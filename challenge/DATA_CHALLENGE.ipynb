{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA_CHALLENGE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsMvGp1sOiUM1I7eIKrbJr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matancoo/IML.HUJI/blob/main/challenge/DATA_CHALLENGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import plotly.graph_objects as go\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# from kmodes.kprototypes import KPrototypes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random \n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "faVZjc4rXodq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #helper functions\n",
        "# def split_train_test(X: pd.DataFrame, y: pd.Series, train_proportion: float = .25): \n",
        "    \n",
        "#     \"\"\"\n",
        "#     Split given sample to a training- and testing sample\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     X : DataFrame of shape (n_samples, n_features)\n",
        "#         Data frame of samples and feature values.\n",
        "\n",
        "#     y : Series of shape (n_samples, )\n",
        "#         Responses corresponding samples in data frame.\n",
        "\n",
        "#     train_proportion: Fraction of samples to be split as training set\n",
        "\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "# #we will first suffle and then take first frac.\n",
        "#     n_samples = y.shape[0]\n",
        "#     p = np.ceil(train_proportion * n_samples).astype(int)\n",
        "#     full_set = pd.concat([X,y],axis=1).sample(frac=1)\n",
        "\n",
        "#     train_X= full_set.iloc[:p, :]\n",
        "#     test_X = full_set.iloc[p:,:]\n",
        "#     train_y = train_X.pop(train_X.columns[-1])\n",
        "#     test_y = test_X.pop(test_X.columns[-1])\n",
        "\n",
        "\n",
        "\n",
        "    # return train_X,train_y,test_X,test_y\n",
        "\n"
      ],
      "metadata": {
        "id": "iVu9CBdCDoDb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KmUN5nNrTgQM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def load_data(filename: str):\n",
        "    \"\"\"\n",
        "    Load Agoda booking cancellation dataset\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename: str\n",
        "        Path to house prices dataset\n",
        "    Returns\n",
        "    -------\n",
        "    Design matrix and response vector in either of the following formats:\n",
        "    1) Single dataframe with last column representing the response\n",
        "    2) Tuple of pandas.DataFrame and Series\n",
        "    3) Tuple of ndarray of shape (n_samples, n_features) and ndarray of shape (n_samples,)\n",
        "    \"\"\"\n",
        "    # Data Preprocessing\n",
        "    data = pd.read_csv(filename).drop_duplicates()\n",
        "\n",
        "    # reformat the feature names:   #TODO: doesnt work\n",
        "    # def camel_to_snake(name):\n",
        "    #     name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
        "    #     return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()\n",
        "    # # Reformating the feature names: \n",
        "    # data.columns = data.columns.map(lambda x : x.strip())\n",
        "\n",
        "\n",
        "   \n",
        "    #feature engineering before datetime conversion\n",
        "    data['is_cancelled'] = (~pd.isna(data.cancellation_datetime)) #new response\n",
        "    \n",
        "\n",
        "\n",
        "    # creating one hot representation\n",
        "    data = pd.get_dummies(data, columns=['charge_option'])\n",
        "\n",
        "    # Changing dates to datetime type: \n",
        "    data['cancellation_datetime'] = pd.to_datetime(data['cancellation_datetime'])\n",
        "    data['booking_datetime'] = pd.to_datetime(data['booking_datetime'])\n",
        "    data['checkout_date'] = pd.to_datetime(data['checkout_date'])\n",
        "    data['hotel_live_date'] = pd.to_datetime(data['hotel_live_date'])\n",
        "    data['checkin_date'] = pd.to_datetime(data['checkin_date'])\n",
        "\n",
        "    #feature engineering after datetime conversion\n",
        "    data['time_interval'] = abs(data.booking_datetime.dt.day - data.checkin_date.dt.day).astype(int)\n",
        "    \n",
        "    #nationalities --- take only relevant features\n",
        "\n",
        "    # sample dropping\n",
        "    data.dropna(subset = ['origin_country_code','origin_country_code'], inplace = True)\n",
        "    \n",
        "    # check that check-in is before the end_date\n",
        "    end_date = pd.to_datetime('2018-12-13')\n",
        "    data = data[data.checkin_date <= end_date]\n",
        "\n",
        "    #feature dropping:\n",
        "    data.drop(columns=['booking_datetime', 'checkin_date','checkout_date','cancellation_datetime','hotel_id','h_booking_id','h_customer_id','hotel_brand_code','hotel_area_code','hotel_live_date','hotel_city_code','cancellation_policy_code','original_payment_currency','original_payment_type','original_payment_method','language','origin_country_code','guest_nationality_country_name','customer_nationality','accommadation_type_name','hotel_country_code'],inplace=True)\n",
        "#not final--> need to do k-prototype and further data featuring\n",
        "\n",
        "\n",
        "    #replaceing nan with 0.0\n",
        "\n",
        "    data['request_nonesmoke'].replace(to_replace=np.nan, value = 0.0, inplace= True)\n",
        "    data['request_latecheckin'].replace(to_replace=np.nan, value=0.0, inplace = True)\n",
        "    data['request_highfloor'].replace(to_replace=np.nan, value=0.0, inplace = True)\n",
        "    data['request_largebed'].replace(to_replace=np.nan, value=0.0, inplace = True)\n",
        "    data['request_twinbeds'].replace(to_replace=np.nan, value=0.0, inplace = True)\n",
        "    data['request_airport'].replace(to_replace=np.nan, value=0.0, inplace = True)\n",
        "    data['request_earlycheckin'].replace(to_replace=np.nan, value=0.0, inplace = True)\n",
        "\n",
        "    # data.cancellation_datetime = data['cancellation_datetime'].apply(lambda x: x.date())\n",
        "    # start_date = pd.to_datetime('2018-12-07')\n",
        "    # end_date = pd.to_datetime('2018-12-13')\n",
        "    # df['cancellation_datetime'] = (df.cancellation_datetime <= end_date) &  (df.cancellation_datetime > start_date)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Dropping samples that have a checkin-date after 2018-12-13\n",
        "    #Turning objects to one-hot vectors:\n",
        "    # labels = data.pop(\"cancellation_datetime\")\n",
        "    # features = data\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_and_export(estimator, X: np.ndarray, filename: str):\n",
        "    \"\"\"\n",
        "    Export to specified file the prediction results of given estimator on given testset.\n",
        "    File saved is in csv format with a single column named 'predicted_values' and n_samples rows containing\n",
        "    predicted values.\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator: BaseEstimator or any object implementing predict() method as in BaseEstimator (for example sklearn)\n",
        "        Fitted estimator to use for prediction\n",
        "    X: ndarray of shape (n_samples, n_features)\n",
        "        Test design matrix to predict its responses\n",
        "    filename:\n",
        "        path to store file at\n",
        "    \"\"\"\n",
        "    pd.DataFrame(estimator.predict(X), columns=[\"predicted_values\"]).to_csv(filename, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "H4DvPPeYaVqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.parsers.readers import read_csv\n",
        "#loading data:\n",
        "data  = load_data('https://raw.githubusercontent.com/Matancoo/IML.HUJI/main/datasets/agoda_cancellation_train.csv')\n"
      ],
      "metadata": {
        "id": "q3yzm85gXmgP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-qhHWrXnR6o",
        "outputId": "f6e92684-8171-4f37-be0c-518e3ec76b5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hotel_star_rating                float64\n",
              "guest_is_not_the_customer          int64\n",
              "no_of_adults                       int64\n",
              "no_of_children                     int64\n",
              "no_of_extra_bed                    int64\n",
              "no_of_room                         int64\n",
              "original_selling_amount          float64\n",
              "is_user_logged_in                   bool\n",
              "is_first_booking                    bool\n",
              "request_nonesmoke                float64\n",
              "request_latecheckin              float64\n",
              "request_highfloor                float64\n",
              "request_largebed                 float64\n",
              "request_twinbeds                 float64\n",
              "request_airport                  float64\n",
              "request_earlycheckin             float64\n",
              "hotel_chain_code                 float64\n",
              "is_cancelled                        bool\n",
              "charge_option_Pay Later            uint8\n",
              "charge_option_Pay Now              uint8\n",
              "charge_option_Pay at Check-in      uint8\n",
              "time_interval                      int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize cancellation basded on hotel country location\n",
        "\n",
        "# df= data[[\"hotel_country_code\", 'is_cancelled']].groupby([\"hotel_country_code\"])['is_cancelled'].sum().reset_index()\n",
        "# px.bar(df,x=\"hotel_country_code\",y='is_cancelled')\n",
        "\n"
      ],
      "metadata": {
        "id": "EwPVouq0_Wii"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize cancellation basded on clients country location\n",
        "\n",
        "# df= data[[\"customer_nationality\", 'is_cancelled']].groupby(['customer_nationality'])['is_cancelled'].sum().reset_index()\n",
        "# px.bar(df,x=\"customer_nationality\",y='is_cancelled')"
      ],
      "metadata": {
        "id": "_7434oygSn5t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing time interval and booking cancellation\n",
        "\n",
        "# x = data[['time_interval', 'is_cancelled']]\n",
        "# px.bar(x,x='is_cancelled',y='time_interval')"
      ],
      "metadata": {
        "id": "bCGY2GxV6dcn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # returns a correlation matrix of each feature with all others.\n",
        "# # data = df.select_dtypes(include=np.number)  # select only numerical features\n",
        "# corr_matrix = data.corr(method='pearson').loc[:, ['is_cancelled']].sort_values('is_cancelled')\n",
        "# corr_target = abs(corr_matrix)\n",
        "\n",
        "response = data.pop('is_cancelled')\n"
      ],
      "metadata": {
        "id": "kWRYwDsWbS7N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1hUfBR-huhWC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data_utils\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, response)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Scaling the data\n",
        "\n",
        "\n",
        "ss = StandardScaler()\n",
        "X_train = ss.fit_transform(X_train)\n",
        "X_test = ss.transform(X_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(X_train, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(X_test, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Passing to DataLoader\n",
        "#normalization of data before NN\n",
        "# Creating our model's structure\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) #can we use flatten here to guess the input dimention to the linear layer?\n",
        "        # self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) \n",
        "       \n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x \n",
        "net = Net()\n",
        "\n",
        "\n",
        "EPOCH = 15\n",
        "train_losses =[]\n",
        "test_losses =[]\n",
        "\n",
        "\n",
        "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
        "\n",
        "#training\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for data in trainloader:\n",
        "        print(data.shape)\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #Train loss\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss/len(trainloader)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "#testing\n",
        "\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for data in testloader:\n",
        "        images, labels = data \n",
        "\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs,labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    #Test loss\n",
        "    epoch_loss = running_loss/len(testloader)\n",
        "    test_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting training and testing losses\n",
        " \n",
        "x = np.linspace(1,EPOCH,EPOCH).astype(int)\n",
        "y_train = np.array(train_losses)\n",
        "y_test = np.array(test_losses)\n",
        "\n",
        "plt.plot(x,y_train, 'r')\n",
        "plt.plot(x,y_test,'b')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "# Fitting the model\n",
        "history = model.fit(X_train,\n",
        "                    y_train, \n",
        "                    batch_size = 256,\n",
        "                    validation_data =(X_test, y_test),\n",
        "                    epochs = 15,\n",
        "                    verbose = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HAZjDO9QXlNy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "191729b1-2d28-40c1-f21c-cb150ebac351"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 21])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a2191ef188d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out our train loss and test loss over epochs.\n",
        "train_loss = history.history['loss']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "# Visualizing our training and testing loss by epoch\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
        "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
        "plt.title('Training and Testing Loss by Epoch', fontsize = 20)\n",
        "plt.xlabel('Epoch', fontsize = 11)\n",
        "plt.ylabel('Binary Crossentropy', fontsize = 11)\n",
        "plt.legend(fontsize = 11);\n",
        "\n",
        "# Credit to GA CNN global lecture author for the graph code "
      ],
      "metadata": {
        "id": "WXEC4c3ScIGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# if __name__ == '__main__':\n",
        "#     np.random.seed(0)\n",
        "\n",
        "#     # Load data\n",
        "#     df, cancellation_labels = load_data(\"../datasets/agoda_cancellation_train.csv\")\n",
        "#     train_X, train_y, test_X, test_y = split_train_test(df, cancellation_labels)\n",
        "\n",
        "#     # Fit model over data\n",
        "#     estimator = AgodaCancellationEstimator().fit(train_X, train_y)\n",
        "\n",
        "#     # Store model predictions over test set\n",
        "#     evaluate_and_export(estimator, test_X, \"id1_id2_id3.csv\")"
      ],
      "metadata": {
        "id": "O40X2ZolcCvd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}